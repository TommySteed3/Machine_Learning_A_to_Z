{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEEP LEARNING**  \n",
    "* What is Deep Learning?  \n",
    "* Need A LOT of data and A LOT of processing power - which we didn't have until recently.  \n",
    "* This is why ANN/CNN etc were invented awhile ago - but only catching on recently.  \n",
    "* Geoffrey Hinton:  Godfather of deep learning - researched it heavily in the 80s.  Good videos.  \n",
    "* Attempt to mimic how the human brain functions.  \n",
    "* Brain has 100B neurons - each is connected to ~ 1,000 neighbors  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Deep Learning Attempts to mimic the brain:  \n",
    "* Input Layer  \n",
    "* Hidden Layer  \n",
    "* Output Layer  \n",
    "* Each input layer is connected to each hidden layer  \n",
    "* Multiple Hidden layers - all hidden layers are connected  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOPICS FOR THIS SECTION - PLAN OF ATTACK**  \n",
    "* Neuron  \n",
    "* Activation Function  \n",
    "* How do NN work?  (Example)  \n",
    "* How do NN learn?  \n",
    "* Gradient Descent  \n",
    "* Stochastic Gradient Descent  \n",
    "* Backpropagation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEURON**  \n",
    "* Basic building block of NNs  \n",
    "1.  Input Values (1-N):  These are the N Independent Variables used in the model for a specific observation  \n",
    "* Need to standardize or normalize  \n",
    "2. Neuron: Hidden Layers  \n",
    "3. Output:  Can be numerical ; categorical ; categorical  \n",
    "* The above is an example of a single observation  \n",
    "4. Synapses:  Connections between Input Value and Neuron (Hidden Layers)  \n",
    "* Assigned weights:  these get adjusted through the process of learning  \n",
    "* This is where Gradient Descent and Backward Propagation come into play  \n",
    "**WHAT HAPPENS INSIDE THE NEURON?**  \n",
    "1.  Sum product of input values and weights for each observation.  \n",
    "2.  Applies Activation Function to the above sumproduct.  \n",
    "3.  Depending on this result - value may or may not get passed to next level  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACTIVATION FUNCTION**  \n",
    "* There are several options - we will look at 4 primary:  \n",
    "1. Threshold Function:  1 if x above a certain value - otherwise 0  \n",
    "2. Sigmoid Function:  1/(1 + exp(-x))  Curve from 0 to 1 from logistic ; unlike threshold - this is smooth  \n",
    "3. Rectifier: max(x,0) ; kink at 0 then linear with x ; very popular even with discontinuity at 0  \n",
    "4. Hyperbolic Tangent (tanh): Similar to Sigmoid ; Looks like Sigmoid - but Y values go -1 to 1 instead of 0 to 1  \n",
    "* [(1-exp(-2x))]/[(1+exp(-2x))]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HOW DO NNs WORK?**  \n",
    "* Example:  Predict a House Price ; 4 Input Variables ; Area - Bedrooms - Distance to City (Miles) - Age  \n",
    "* the 4 variables above are our input layer for each observation  \n",
    "* Simplest form:  No hiden layer ; Price = weighted sum of inputs.  \n",
    "* Very powerful - even without hidden layers there is some value - but hidden layers create much more powerful performance  \n",
    "* All inputs connected to each node in the hidden layer - but some weights might be 0  \n",
    "* Different subsets of inputs have non-zero weights for the various neurons  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HOW DO NNs LEARN?**  \n",
    "* 2 ways for a machine to learn:  \n",
    "A. Hard code all the rules  \n",
    "B. NN - provide a framework and let the algo learn  \n",
    "* Ultimately - NN will learn - and you might not know what rules it figured out / is using  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start with single layer:  'Single Layer Feed Forward NN' also called a perceptron - output is y-hat  \n",
    "* For NN to learn  needs to compare y-hat with y  \n",
    "* Cost Function:  C = 1/2(y-hat - y)^2  ; there are many options - but this is common and will be reviewed more w gradient descent  \n",
    "* Feed the cost function back into the NN - to adjust the weights  - goal is to ultimately minimize the cost function  \n",
    "* For this example - dealing with a single row (observation) - but several associated independent RVs  \n",
    "WHAT ABOUT MULTIPLE ROWS?  \n",
    "* For example - use the same perceptron for each  \n",
    "* One epoch = go through a whole dataset and train NN on all rows  \n",
    "* Start off - get your y-hat for all - compare to actual y for all and have cost function for entire dataset  \n",
    "* Now you update the weights - same weights for all the rows - trying to minimize overall cost function  \n",
    "* This whole process = BACK PROPAGATION: Sum of Squares difference is calculated and back propagated through NN to update weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRADIENT DESCENT**  \n",
    "* How can we minimize cost function during back propagation?  \n",
    "* One example - brute force method - trial and error  \n",
    "* Curse of dimensionality: Multiple layers and independent variables: weights grow exponentially  \n",
    "* Gradient Descent is a method to make this more efficient  \n",
    "* Differentiate slope wrt target variable - and adjust directionally based on this result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOCHASTIC GRADIENT DESCENT**  \n",
    "* Gradient Descent - requires Cost Function to be Convex (have a single global minimum)  \n",
    "* Not Convex: Gradient Descent might find a local min that is not a global min  \n",
    "* Stochastic Gradient Descent does not require convexity  \n",
    "* Gradient Descent above - looked at all rows at once - also called Batch Gradient Descent  \n",
    "* Stochastic Gradient Descent - look at rows one by one  \n",
    "* Stochastic Gradient Descent helps avoid local mins - because it has higher fluctuations  \n",
    "* Stochastic Gradient Descent is faster (somewhat counterintuitive)  \n",
    "* Batch Gradient Descent is Deterministic where Stochastic is Random - results are stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BACKPROPAGATION**  \n",
    "* Forward Propagation - data is entered into our algo (NN) - hidden layers - and results in a prediction  \n",
    "* Backpropagation is where actual/expected fed back into the algo  \n",
    "* ALL WEIGHTS ARE ADJUSTED AT THE SAME TIME!!!  THIS IS THE STRENGTH OF BACKPROPAGATION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING THE ANN WITH STOCHASTIC GRADIENT DESCENT**   \n",
    "1. Randomly initialize weights to values close to 0 (but not 0).  \n",
    "2. Input first observation of your dataset in the input layer - each feature in one input node  \n",
    "3. Forward-Propagation:  From L-R the neurons are activated in a way that the impact of each neuron's activation is limited by the weights.  Propagate the activations until getting the predicted result y.  \n",
    "4. Compare the predicted result to the actual result.  Measure the generated error.  \n",
    "5. Back-Propagation: From R-L the error is back propagated.  Upate the weights according to how much they are responsible for the error.  Learning rate decides by how much we upate the weights.  \n",
    "6. Repeat Steps 1-5 and update weights after each observation (Reinforcement Learning) OR  \n",
    "   Repeat Steps 1-5 but update weights only after a batch of observations (Batch Learning)  \n",
    "7. When whole training set passes through ANN - that makes an epoch.  Redo more epochs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAMPLE BUSINESS PROBLEM**  \n",
    "* Simulated bank data - look at who might leave (noticed high churn rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         2 hours 44 minutes \n",
      "    H2O cluster timezone:       America/Los_Angeles \n",
      "    H2O data parsing timezone:  UTC \n",
      "    H2O cluster version:        3.24.0.5 \n",
      "    H2O cluster version age:    1 year, 9 months and 7 days !!! \n",
      "    H2O cluster name:           H2O_started_from_R_tgsxp_cti312 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   3.37 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.6.1 (2019-07-05) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in h2o.clusterInfo():\n",
      "\"\n",
      "Your H2O cluster version is too old (1 year, 9 months and 7 days)!\n",
      "Please download and install the latest version from http://h2o.ai/download/\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)? Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = read.csv('Churn_Modelling.csv')\n",
    "dataset = dataset[4:14]\n",
    "\n",
    "# Encoding the categorical variables as factors\n",
    "dataset$Geography = as.numeric(factor(dataset$Geography,\n",
    "                                      levels = c('France', 'Spain', 'Germany'),\n",
    "                                      labels = c(1, 2, 3)))\n",
    "dataset$Gender = as.numeric(factor(dataset$Gender,\n",
    "                                   levels = c('Female', 'Male'),\n",
    "                                   labels = c(1, 2)))\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "# install.packages('caTools')\n",
    "library(caTools)\n",
    "set.seed(123)\n",
    "split = sample.split(dataset$Exited, SplitRatio = 0.8)\n",
    "training_set = subset(dataset, split == TRUE)\n",
    "test_set = subset(dataset, split == FALSE)\n",
    "\n",
    "# Feature Scaling\n",
    "training_set[-11] = scale(training_set[-11])\n",
    "test_set[-11] = scale(test_set[-11])\n",
    "\n",
    "# Fitting ANN to the Training set\n",
    "# install.packages('h2o')\n",
    "library(h2o)\n",
    "h2o.init(nthreads = -1)\n",
    "model = h2o.deeplearning(y = 'Exited',\n",
    "                         training_frame = as.h2o(training_set),\n",
    "                         activation = 'Rectifier',\n",
    "                         hidden = c(5,5),\n",
    "                         epochs = 100,\n",
    "                         train_samples_per_iteration = -2)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = h2o.predict(model, newdata = as.h2o(test_set[-11]))\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred = as.vector(y_pred)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = table(test_set[, 11], y_pred)\n",
    "\n",
    " h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   y_pred\n",
       "       0    1\n",
       "  0 1545   48\n",
       "  1  229  178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
